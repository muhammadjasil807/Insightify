{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xhKnaffscsk"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Install Required Packages\n",
        "\n",
        "# Install core ML, NLP, and data handling packages silently\n",
        "!pip install -q scikit-learn pandas joblib numpy nltk spacy transformers torch emoji streamlit pyngrok\n",
        "\n",
        "# Download SpaCy English model (small) for NER\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Download NLTK resources for Twitter dataset & text processing\n",
        "import nltk\n",
        "nltk.download('stopwords')       # Stopwords for text cleaning\n",
        "nltk.download('punkt')           # Tokenizer\n",
        "nltk.download('twitter_samples') # Sample tweets dataset\n",
        "\n",
        "#Explanation:\n",
        "#Installs all necessary packages for ML, NLP, and deployment.\n",
        "#Downloads SpaCy model en_core_web_sm for named entity recognition.\n",
        "#Downloads NLTK datasets for sentiment analysis and text cleaning.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Import Required Libraries\n",
        "\n",
        "# Data Handling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import random\n",
        "import re\n",
        "\n",
        "# NLP\n",
        "from nltk.corpus import twitter_samples, stopwords\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import spacy\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# ML Models\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Streamlit & Deployment\n",
        "import streamlit as st\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Global Settings\n",
        "RANDOM_STATE = 42  # Ensures reproducibility\n",
        "\n",
        "#Explanation:\n",
        "#Organized imports into categories: data handling, NLP, ML, deployment.\n",
        "#RANDOM_STATE is defined globally for reproducibility."
      ],
      "metadata": {
        "id": "eyLH6EpQs-zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Text Cleaning Function\n",
        "\n",
        "# Initialize TweetTokenizer for tokenizing tweets\n",
        "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "stop_words = set(stopwords.words('english'))  # English stopwords\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans input text by:\n",
        "    1. Removing URLs and mentions\n",
        "    2. Tokenizing using TweetTokenizer\n",
        "    3. Removing stopwords\n",
        "    4. Returning cleaned string\n",
        "    \"\"\"\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # Remove URLs\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)            # Remove @mentions\n",
        "    tokens = tokenizer.tokenize(text)           # Tokenize text\n",
        "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]  # Remove stopwords & non-alpha\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "#Explanation:\n",
        "#Central text cleaning function used for all datasets.\n",
        "#Removes noise from text like URLs, mentions, and stopwords.\n",
        "#Keeps only alphabetical words."
      ],
      "metadata": {
        "id": "6fyY6KbnvM8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Sentiment Dataset (NLTK Twitter)\n",
        "\n",
        "# Load positive and negative tweets\n",
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "# Create DataFrame\n",
        "df_sentiment = pd.DataFrame({\n",
        "    'text': positive_tweets + negative_tweets,\n",
        "    'sentiment': ['positive']*len(positive_tweets) + ['negative']*len(negative_tweets)\n",
        "})\n",
        "\n",
        "# Clean text\n",
        "df_sentiment['clean_text'] = df_sentiment['text'].apply(clean_text)\n",
        "\n",
        "# Shuffle and reset index for randomness\n",
        "df_sentiment = df_sentiment.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
        "\n",
        "# Quick check\n",
        "print(\"Sentiment dataset size:\", len(df_sentiment))\n",
        "print(df_sentiment['sentiment'].value_counts())\n",
        "df_sentiment.head()\n",
        "\n",
        "#Explanation:\n",
        "#Loads NLTK Twitter dataset for sentiment analysis.\n",
        "#Applies clean_text() for preprocessing.\n",
        "#Shuffles dataset to remove ordering bias.\n"
      ],
      "metadata": {
        "id": "zFyOivQkvhlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Intent Dataset\n",
        "\n",
        "# Define small intent dataset\n",
        "data_intent = {\n",
        "    \"text\": [\n",
        "        \"hello there\",\"hi buddy\",\"good morning\",\"ok bye\",\"see you soon\",\n",
        "        \"what time is the meeting\",\"can you help me\",\"i feel sad today\",\n",
        "        \"great job team\",\"i reached the office\",\"the package is on the table\"\n",
        "    ],\n",
        "    \"intent\": [\n",
        "        \"greeting\",\"greeting\",\"greeting\",\"goodbye\",\"goodbye\",\n",
        "        \"question\",\"question\",\"complaint\",\"praise\",\"info\",\"info\"\n",
        "]\n",
        "}\n",
        "\n",
        "df_intent = pd.DataFrame(data_intent)\n",
        "df_intent['clean_text'] = df_intent['text'].apply(clean_text)\n",
        "\n",
        "# Quick check\n",
        "print(\"Intent dataset size:\", len(df_intent))\n",
        "print(df_intent['intent'].value_counts())\n",
        "df_intent.head()\n",
        "\n",
        "#Explanation:\n",
        "#Small custom dataset for intent classification.\n",
        "#Applies the same cleaning function for consistency.\n"
      ],
      "metadata": {
        "id": "UeX1DueXvzv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Large Synthetic Intent Dataset (df_big)\n",
        "\n",
        "# Define phrases per intent\n",
        "phrases_dict = {\n",
        "    \"greeting\": [\"hello there\", \"hi friend\", \"hey buddy\", \"good morning\", \"good evening\", \"hello how are you\", \"hi what's up\", \"hey there\"],\n",
        "    \"goodbye\": [\"bye take care\", \"see you soon\", \"talk later\", \"good night\", \"i have to go now\", \"catch you later\", \"see you tomorrow\"],\n",
        "    \"question\": [\"can you help me\", \"what time is the meeting\", \"how does this work\", \"where can i find it\", \"is anyone available\", \"can i ask something\", \"what should i do\", \"where do i submit\"],\n",
        "    \"complaint\": [\"i feel tired and low today\", \"this app is frustrating\", \"my order arrived broken\", \"i am not happy with this\", \"this is really disappointing\", \"everything is going wrong\", \"i am feeling stressed\", \"the service is terrible\", \"i hate this situation\"],\n",
        "    \"praise\": [\"thanks so much\", \"great job team\", \"i love this feature\", \"this is awesome\", \"amazing experience\", \"you did wonderful work\", \"nice work\", \"really helpful\"],\n",
        "    \"info\": [\"i reached the office\", \"the delivery came today\", \"i will join soon\", \"i am at home now\", \"package is on the table\", \"meeting has started\", \"i am ready for the call\"]\n",
        "}\n",
        "\n",
        "# Generate 300 examples per intent with minor punctuation variations\n",
        "dataset = []\n",
        "for intent, phrases in phrases_dict.items():\n",
        "    for _ in range(300):\n",
        "        text = random.choice(phrases) + \" \" + random.choice([\"\", \"!\", \".\", \"??\", \"!!\"])\n",
        "        dataset.append([text, intent])\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_big = pd.DataFrame(dataset, columns=[\"text\", \"intent\"])\n",
        "df_big['clean_text'] = df_big['text'].apply(clean_text)\n",
        "\n",
        "print(\"Large intent dataset (df_big) created. Total examples:\", len(df_big))\n",
        "df_big['intent'].value_counts()\n",
        "\n",
        "#Explanation:\n",
        "#Expands intent dataset to large synthetic dataset for better model training.\n",
        "#Introduces minor punctuation variations to simulate real user inputs.\n"
      ],
      "metadata": {
        "id": "nXIFSnG5wTZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Train Sentiment Classifier\n",
        "\n",
        "# Split features and labels\n",
        "X_sent = df_sentiment['clean_text'].values\n",
        "y_sent = df_sentiment['sentiment'].values\n",
        "\n",
        "# Train-test split\n",
        "X_train_sent, X_test_sent, y_train_sent, y_test_sent = train_test_split(\n",
        "    X_sent, y_sent, test_size=0.2, random_state=RANDOM_STATE, stratify=y_sent\n",
        ")\n",
        "\n",
        "# TF-IDF vectorization (unigrams + bigrams)\n",
        "tfidf_sent = TfidfVectorizer(ngram_range=(1,2), min_df=2, max_df=0.95)\n",
        "X_train_sent_tfidf = tfidf_sent.fit_transform(X_train_sent)\n",
        "X_test_sent_tfidf = tfidf_sent.transform(X_test_sent)\n",
        "\n",
        "# Logistic Regression classifier\n",
        "clf_sent = LogisticRegression(max_iter=200, random_state=RANDOM_STATE)\n",
        "clf_sent.fit(X_train_sent_tfidf, y_train_sent)\n",
        "\n",
        "# Evaluate\n",
        "y_pred_sent = clf_sent.predict(X_test_sent_tfidf)\n",
        "acc_sent = accuracy_score(y_test_sent, y_pred_sent)\n",
        "print(f\"Sentiment Accuracy: {acc_sent:.4f}\")\n",
        "print(classification_report(y_test_sent, y_pred_sent, digits=4))\n",
        "\n",
        "# Save model and vectorizer\n",
        "joblib.dump(tfidf_sent, 'tfidf_sent.joblib')\n",
        "joblib.dump(clf_sent, 'clf_sent.joblib')\n",
        "\n",
        "#Explanation:\n",
        "#TF-IDF vectorization converts text to numeric features.\n",
        "#Logistic Regression trained on sentiment data.\n",
        "#Evaluates model and saves for Streamlit usage.\n"
      ],
      "metadata": {
        "id": "4iXXrETawhu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Train Intent Classifier (df_big)\n",
        "\n",
        "# Features and labels\n",
        "X_intent = df_big['clean_text'].values\n",
        "y_intent = df_big['intent'].values\n",
        "\n",
        "# Train-test split\n",
        "X_train_int, X_test_int, y_train_int, y_test_int = train_test_split(\n",
        "    X_intent, y_intent, test_size=0.2, random_state=RANDOM_STATE, stratify=y_intent\n",
        ")\n",
        "\n",
        "# TF-IDF vectorization\n",
        "tfidf_int = TfidfVectorizer(ngram_range=(1,2), min_df=1, max_df=0.95)\n",
        "X_train_int_tfidf = tfidf_int.fit_transform(X_train_int)\n",
        "X_test_int_tfidf = tfidf_int.transform(X_test_int)\n",
        "\n",
        "# Logistic Regression classifier\n",
        "clf_int = LogisticRegression(max_iter=300, random_state=RANDOM_STATE)\n",
        "clf_int.fit(X_train_int_tfidf, y_train_int)\n",
        "\n",
        "# Evaluate\n",
        "y_pred_int = clf_int.predict(X_test_int_tfidf)\n",
        "acc_int = accuracy_score(y_test_int, y_pred_int)\n",
        "print(f\"Intent Classification Accuracy: {acc_int:.4f}\")\n",
        "print(classification_report(y_test_int, y_pred_int, digits=4))\n",
        "\n",
        "# Save model and vectorizer\n",
        "joblib.dump(tfidf_int, 'tfidf_int.joblib')\n",
        "joblib.dump(clf_int, 'clf_int.joblib')\n",
        "\n",
        "#Explanation:\n",
        "#Trains a robust intent classifier on synthetic dataset.\n",
        "#Saves vectorizer and model for deployment."
      ],
      "metadata": {
        "id": "U4SP8cERxnL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: HuggingFace Emotion Model\n",
        "\n",
        "# Load pre-trained emotion detection model\n",
        "emotion_model_name = \"SamLowe/roberta-base-go_emotions\"\n",
        "tokenizer_emotion = AutoTokenizer.from_pretrained(emotion_model_name)\n",
        "model_emotion = AutoModelForSequenceClassification.from_pretrained(emotion_model_name)\n",
        "id2label_emotion = model_emotion.config.id2label\n",
        "\n",
        "def predict_emotion(text):\n",
        "    \"\"\"\n",
        "    Predict top emotion and probability scores for all emotions.\n",
        "    \"\"\"\n",
        "    if not text.strip():\n",
        "        return \"No text provided\", {}\n",
        "    inputs = tokenizer_emotion(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model_emotion(**inputs)\n",
        "    probs = torch.softmax(outputs.logits, dim=1)[0].numpy()\n",
        "    top_idx = int(np.argmax(probs))\n",
        "    top_emotion = id2label_emotion[top_idx]\n",
        "    scores = {id2label_emotion[i]: float(probs[i]) for i in range(len(probs))}\n",
        "    return top_emotion, scores\n",
        "\n",
        "\n",
        "#Explanation:\n",
        "#Uses GoEmotions model from HuggingFace.\n",
        "#Returns top emotion and probabilities for all emotions."
      ],
      "metadata": {
        "id": "3iC3lLuSx43i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: HuggingFace Toxicity Detection\n",
        "\n",
        "toxicity_model_name = \"unitary/toxic-bert\"\n",
        "tokenizer_toxic = AutoTokenizer.from_pretrained(toxicity_model_name)\n",
        "model_toxic = AutoModelForSequenceClassification.from_pretrained(toxicity_model_name)\n",
        "tox_labels = ['toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "def predict_toxicity(text):\n",
        "    \"\"\"\n",
        "    Predicts probability for toxicity labels.\n",
        "    \"\"\"\n",
        "    if not text.strip():\n",
        "        return {}\n",
        "    inputs = tokenizer_toxic(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model_toxic(**inputs)\n",
        "    probs = torch.sigmoid(outputs.logits)[0].numpy()\n",
        "    return {tox_labels[i]: float(probs[i]) for i in range(len(tox_labels))}\n",
        "\n",
        "#Explanation:\n",
        "#Multi-label classification for toxicity detection.\n",
        "#Uses sigmoid because a text can have multiple toxicity labels.\n"
      ],
      "metadata": {
        "id": "Zu3EGAiY0AgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Named Entity Recognition (NER) using SpaCy\n",
        "\n",
        "nlp_ner = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_entities(text):\n",
        "    \"\"\"\n",
        "    Extracts named entities from text (PERSON, ORG, GPE, etc.)\n",
        "    \"\"\"\n",
        "    if not text.strip():\n",
        "        return {}\n",
        "    doc = nlp_ner(text)\n",
        "    entities = {}\n",
        "    for ent in doc.ents:\n",
        "        entities.setdefault(ent.label_, []).append(ent.text)\n",
        "    return entities\n",
        "\n",
        "#Explanation:\n",
        "#Extracts named entities using SpaCy small model.\n",
        "#Returns dictionary {entity_type: [entities]}."
      ],
      "metadata": {
        "id": "xhNDf98L0xW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "import spacy\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# ---------------------------\n",
        "# Installation Checks\n",
        "# ---------------------------\n",
        "def install_package(package):\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "try:\n",
        "    import spacy_transformers\n",
        "except ImportError:\n",
        "    install_package(\"spacy[transformers]\")\n",
        "\n",
        "# ---------------------------\n",
        "# Load SpaCy NER Model\n",
        "# ---------------------------\n",
        "try:\n",
        "    nlp_ner = spacy.load(\"en_core_web_trf\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: en_core_web_trf failed ({e}), falling back to en_core_web_sm.\")\n",
        "    try:\n",
        "        nlp_ner = spacy.load(\"en_core_web_sm\")\n",
        "    except OSError:\n",
        "        install_package(\"en_core_web_sm\")\n",
        "        nlp_ner = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# ---------------------------\n",
        "# Load Models\n",
        "# ---------------------------\n",
        "sentiment_model = pipeline(\"sentiment-analysis\")\n",
        "emotion_model_name = \"j-hartmann/emotion-english-distilroberta-base\"\n",
        "emotion_tokenizer = AutoTokenizer.from_pretrained(emotion_model_name)\n",
        "emotion_model = AutoModelForSequenceClassification.from_pretrained(emotion_model_name)\n",
        "emotion_id2label = emotion_model.config.id2label\n",
        "\n",
        "toxicity_model_name = \"unitary/toxic-bert\"\n",
        "tox_tokenizer = AutoTokenizer.from_pretrained(toxicity_model_name)\n",
        "tox_model = AutoModelForSequenceClassification.from_pretrained(toxicity_model_name)\n",
        "\n",
        "# ---------------------------\n",
        "# Helper Functions\n",
        "# ---------------------------\n",
        "def analyze_sentiment(text):\n",
        "    result = sentiment_model(text)[0]\n",
        "    label = result['label']\n",
        "    color = \"green\" if label.lower() == \"positive\" else \"red\"\n",
        "    return label.capitalize(), color\n",
        "\n",
        "def analyze_emotion(text, threshold=0.1):\n",
        "    inputs = emotion_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = emotion_model(**inputs)\n",
        "    probs = torch.softmax(outputs.logits, dim=1)[0].cpu().numpy()\n",
        "    emotions = []\n",
        "    for idx, prob in enumerate(probs):\n",
        "        if prob >= threshold:\n",
        "            emotion = emotion_id2label[idx]\n",
        "            if emotion.lower() in [\"joy\", \"love\", \"relief\", \"admiration\", \"optimism\", \"excited\", \"surprise\"]:\n",
        "                color = \"green\"\n",
        "            elif emotion.lower() in [\"neutral\", \"curiosity\"]:\n",
        "                color = \"gray\"\n",
        "            else:\n",
        "                color = \"red\"\n",
        "            emotions.append((emotion.capitalize(), color))\n",
        "    return emotions if emotions else [(\"Neutral\", \"gray\")]\n",
        "\n",
        "def analyze_toxicity(text):\n",
        "    inputs = tox_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = tox_model(**inputs)\n",
        "    probs = torch.sigmoid(outputs.logits)[0].cpu().numpy()\n",
        "    score = max(probs)\n",
        "    if score < 0.2:\n",
        "        return \"Low\", \"green\"\n",
        "    elif score < 0.5:\n",
        "        return \"Moderate\", \"orange\"\n",
        "    else:\n",
        "        return \"High\", \"red\"\n",
        "\n",
        "def extract_entities(text):\n",
        "    doc = nlp_ner(text)\n",
        "    entities = {}\n",
        "    for ent in doc.ents:\n",
        "        entities.setdefault(ent.label_, []).append(ent.text)\n",
        "    keywords = {\"PRODUCT\": [\"phone\", \"laptop\", \"computer\", \"tablet\", \"watch\"]}\n",
        "    for label, words in keywords.items():\n",
        "        for word in words:\n",
        "            if word.lower() in text.lower():\n",
        "                entities.setdefault(label, []).append(word)\n",
        "    return entities if entities else None\n",
        "\n",
        "def detect_intent(text):\n",
        "    text_lower = text.lower()\n",
        "    greetings = [\"hi\", \"hello\", \"hey\", \"good morning\", \"good evening\"]\n",
        "    questions = [\"who\", \"what\", \"where\", \"when\", \"why\", \"how\", \"?\"]\n",
        "    complaints = [\"not working\", \"problem\", \"issue\", \"tired\", \"can't\", \"cannot\", \"error\",\n",
        "                  \"fail\", \"crash\", \"frustrated\", \"hate\", \"annoyed\"]\n",
        "    if any(word in text_lower for word in complaints):\n",
        "        return \"Complaint\"\n",
        "    elif any(text_lower.startswith(q) or q in text_lower for q in questions):\n",
        "        return \"Question\"\n",
        "    elif any(word in text_lower for word in greetings):\n",
        "        return \"Greeting\"\n",
        "    else:\n",
        "        return \"Statement\"\n",
        "\n",
        "def mental_health_insight(text, emotions):\n",
        "    mapping_keywords = {\n",
        "        \"Disappointment / Sadness\": [\"disappointed\", \"sad\", \"frustrated\", \"unhappy\", \"hate\", \"annoyed\"],\n",
        "        \"Excitement / Happiness\": [\"excited\", \"happy\", \"joy\", \"thrilled\", \"optimistic\", \"love\", \"admiration\"],\n",
        "        \"Anxiety / Stress\": [\"anxious\", \"stressed\", \"worried\", \"nervous\", \"tense\", \"overwhelmed\"],\n",
        "        \"Depression\": [\"depressed\", \"hopeless\", \"lonely\", \"worthless\", \"gloomy\"],\n",
        "        \"Phobia / Fear\": [\"afraid\", \"fear\", \"scared\", \"panic\", \"phobia\"],\n",
        "        \"OCD / Compulsive\": [\"repetitive\", \"check\", \"control\", \"ritual\", \"compulsion\"]\n",
        "    }\n",
        "    colors = {\n",
        "        \"Disappointment / Sadness\": \"red\",\n",
        "        \"Excitement / Happiness\": \"green\",\n",
        "        \"Anxiety / Stress\": \"orange\",\n",
        "        \"Depression\": \"darkred\",\n",
        "        \"Phobia / Fear\": \"purple\",\n",
        "        \"OCD / Compulsive\": \"blue\"\n",
        "    }\n",
        "    insights = []\n",
        "    text_lower = text.lower()\n",
        "    for label, keywords in mapping_keywords.items():\n",
        "        if any(w in text_lower for w in keywords):\n",
        "            insights.append((label, colors[label]))\n",
        "    for emo, _ in emotions:\n",
        "        emo_lower = emo.lower()\n",
        "        if emo_lower in [\"anger\", \"sadness\", \"disgust\", \"fear\", \"disappointment\"]:\n",
        "            insights.append((\"Disappointment / Sadness\", colors[\"Disappointment / Sadness\"]))\n",
        "        elif emo_lower in [\"joy\", \"love\", \"relief\", \"optimism\", \"admiration\", \"excited\", \"surprise\"]:\n",
        "            insights.append((\"Excitement / Happiness\", colors[\"Excitement / Happiness\"]))\n",
        "        elif emo_lower in [\"anxiety\", \"stress\", \"worry\", \"nervous\"]:\n",
        "            insights.append((\"Anxiety / Stress\", colors[\"Anxiety / Stress\"]))\n",
        "        elif emo_lower in [\"depression\", \"hopeless\", \"lonely\", \"gloomy\"]:\n",
        "            insights.append((\"Depression\", colors[\"Depression\"]))\n",
        "        elif emo_lower in [\"fear\", \"phobia\", \"panic\", \"scared\"]:\n",
        "            insights.append((\"Phobia / Fear\", colors[\"Phobia / Fear\"]))\n",
        "    # Remove duplicates\n",
        "    seen = set()\n",
        "    unique_insights = []\n",
        "    for label, color in insights:\n",
        "        if label not in seen:\n",
        "            unique_insights.append((label, color))\n",
        "            seen.add(label)\n",
        "    return unique_insights if unique_insights else None\n",
        "\n",
        "def display_entities_with_badges(entities):\n",
        "    if not entities:\n",
        "        return \"None\"\n",
        "    color_map = {\"PERSON\": \"blue\",\"ORG\": \"purple\",\"GPE\": \"teal\",\"LOC\": \"teal\",\"PRODUCT\": \"green\",\n",
        "                 \"DATE\": \"orange\",\"TIME\": \"orange\",\"MISC\": \"gray\"}\n",
        "    badges = []\n",
        "    for label, values in entities.items():\n",
        "        color = color_map.get(label, \"gray\")\n",
        "        for v in values:\n",
        "            badges.append(f\"<span style='color:{color}; font-weight:bold'>{v} ({label})</span>\")\n",
        "    return \" \".join(badges)\n",
        "\n",
        "def display_emotions_with_badges(emotions):\n",
        "    return \" \".join([f\"<span style='color:{color}; font-weight:bold'>{label}</span>\" for label, color in emotions])\n",
        "\n",
        "# ---------------------------\n",
        "# Streamlit UI\n",
        "# ---------------------------\n",
        "st.set_page_config(page_title=\"Insightify\", layout=\"centered\")\n",
        "st.title(\"Insightify\")\n",
        "st.write(\"Analyze **Sentiment, Intent, Emotion, Named Entities, Toxicity, and Mental Health Insights**\")\n",
        "\n",
        "user_input = st.text_area(\"Enter text here:\")\n",
        "\n",
        "serious_toggle = st.checkbox(\"Show only serious mental health concerns (Depression, Phobia, OCD)\")\n",
        "\n",
        "if st.button(\"Analyze\") and user_input.strip():\n",
        "    sentiment, sentiment_color = analyze_sentiment(user_input)\n",
        "    st.markdown(f\"**Sentiment:** <span style='color:{sentiment_color}'>{sentiment}</span>\", unsafe_allow_html=True)\n",
        "\n",
        "    intent = detect_intent(user_input)\n",
        "    st.markdown(f\"**Intent:** {intent}\")\n",
        "\n",
        "    emotions = analyze_emotion(user_input)\n",
        "    st.markdown(\"**Emotion:**\")\n",
        "    st.markdown(display_emotions_with_badges(emotions), unsafe_allow_html=True)\n",
        "\n",
        "    entities = extract_entities(user_input)\n",
        "    st.markdown(\"**Named Entities:**\")\n",
        "    st.markdown(display_entities_with_badges(entities), unsafe_allow_html=True)\n",
        "\n",
        "    tox_level, tox_color = analyze_toxicity(user_input)\n",
        "    st.markdown(f\"**Toxicity:** <span style='color:{tox_color}'>{tox_level}</span>\", unsafe_allow_html=True)\n",
        "\n",
        "    insights = mental_health_insight(user_input, emotions)\n",
        "    st.markdown(\"**Mental Health Insight:**\")\n",
        "    if insights:\n",
        "        if serious_toggle:\n",
        "            serious_insights = [i for i in insights if i[0] in [\"Depression\",\"Phobia / Fear\",\"OCD / Compulsive\"]]\n",
        "            badges = \" \".join([f\"<span style='color:{color}; font-weight:bold'>{label}</span>\" for label, color in serious_insights])\n",
        "            st.markdown(badges if badges else \"None\", unsafe_allow_html=True)\n",
        "        else:\n",
        "            badges = \" \".join([f\"<span style='color:{color}; font-weight:bold'>{label}</span>\" for label, color in insights])\n",
        "            st.markdown(badges, unsafe_allow_html=True)\n",
        "    else:\n",
        "        st.write(\"None\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hpIl0Z70cT6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages (run once)\n",
        "!pip install streamlit pyngrok --quiet\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# Authenticate ngrok\n",
        "NGROK_AUTHTOKEN = \"35h22KzVkgcQxUPw8btKpUyUOa3_3XjmZm3y83CDLiUhebU3c\"\n",
        "ngrok.set_auth_token(NGROK_AUTHTOKEN)\n",
        "\n",
        "# Kill old tunnels / processes\n",
        "ngrok.kill()\n",
        "subprocess.run(\"pkill streamlit\", shell=True)\n",
        "subprocess.run(\"fuser -k 8501/tcp\", shell=True)\n",
        "\n",
        "# Start Streamlit app\n",
        "process = subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE\n",
        ")\n",
        "\n",
        "# Wait for Streamlit to start (poll localhost)\n",
        "max_wait = 60  # seconds\n",
        "start_time = time.time()\n",
        "while True:\n",
        "    try:\n",
        "        r = requests.get(\"http://localhost:8501\")\n",
        "        if r.status_code == 200:\n",
        "            break\n",
        "    except:\n",
        "        pass\n",
        "    if time.time() - start_time > max_wait:\n",
        "        print(\"Streamlit did not start in time. Check app.py for errors.\")\n",
        "        break\n",
        "    time.sleep(1)\n",
        "\n",
        "# Open ngrok tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Your Streamlit app is live at:\", public_url.public_url)\n"
      ],
      "metadata": {
        "id": "56QfochBoJjU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}